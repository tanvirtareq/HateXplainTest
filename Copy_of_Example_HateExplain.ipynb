{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Example_HateExplain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f85d0edf3014cf2856a0da44ddd3d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1711bdf4c934334ba1cd6a818f1c933",
              "IPY_MODEL_7930f6cfe719448f8119d5d08b55cad2"
            ],
            "layout": "IPY_MODEL_62110a53e0ff47589b76ae0419ab2e09"
          }
        },
        "c1711bdf4c934334ba1cd6a818f1c933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f56bf6f9275842878b00bf09f1b25016",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bff3579b80ef465fadcd770541fec81a",
            "value": 1
          }
        },
        "7930f6cfe719448f8119d5d08b55cad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f4288a2816245c88cde3ef65ea4f9fe",
            "placeholder": "​",
            "style": "IPY_MODEL_6e97c34947cd48599fd7a6af06561261",
            "value": " 1/1 [00:38&lt;00:00, 38.02s/it]"
          }
        },
        "62110a53e0ff47589b76ae0419ab2e09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f56bf6f9275842878b00bf09f1b25016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff3579b80ef465fadcd770541fec81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2f4288a2816245c88cde3ef65ea4f9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e97c34947cd48599fd7a6af06561261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvirtareq/HateXplainTest/blob/main/Copy_of_Example_HateExplain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Hqb1OvsXDR",
        "outputId": "dfa30e68-a106-40b1-c90f-d76231aacd8c"
      },
      "source": [
        "!git clone https://github.com/punyajoy/HateXplain.git"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HateXplain'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 386 (delta 73), reused 51 (delta 21), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (386/386), 4.81 MiB | 14.32 MiB/s, done.\n",
            "Resolving deltas: 100% (214/214), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPAJpqcYVEM0"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l4M1_SHtDIm",
        "outputId": "0884474d-47bb-4519-cbba-7589c344b409"
      },
      "source": [
        "cd HateXplain/"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl3PF9alWjHx"
      },
      "source": [
        "!mkdir Saved/\n",
        "!mkdir explanations_dicts/"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU8P_dV5Wnk5",
        "outputId": "d85ab95e-880b-437c-c2f0-6870d3823632"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip  -P Data/"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-02 09:57:45--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2022-04-02 09:57:45--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2022-04-02 09:57:45--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘Data/glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  4.86MB/s    in 5m 54s  \n",
            "\n",
            "2022-04-02 10:03:39 (5.06 MB/s) - ‘Data/glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZi9IlH-W_83",
        "outputId": "7795b4e7-168a-4aeb-c4e6-981916960bc0"
      },
      "source": [
        "!unzip Data/glove.42B.300d.zip -d Data/"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Data/glove.42B.300d.zip\n",
            "  inflating: Data/glove.42B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofRAO-crNgI"
      },
      "source": [
        "!rm Data/glove.42B.300d.zip"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkIaNCQDgvSP",
        "outputId": "6997e863-7cec-4cb8-da42-d2e4a270096c"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: spacy==2.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.3.2)\n",
            "Requirement already satisfied: tqdm==4.43.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.43.0)\n",
            "Requirement already satisfied: Keras==2.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.3.1)\n",
            "Requirement already satisfied: waiting==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: ekphrasis==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: pandas==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.0.3)\n",
            "Requirement already satisfied: transformers==2.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.5.1)\n",
            "Requirement already satisfied: lime==0.2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.2.0.1)\n",
            "Requirement already satisfied: numpy==1.16.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.16.3)\n",
            "Requirement already satisfied: matplotlib==3.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (3.2.1)\n",
            "Requirement already satisfied: gensim==3.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (3.8.1)\n",
            "Requirement already satisfied: neptune_client==0.4.107 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.4.107)\n",
            "Requirement already satisfied: knockknock==0.1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.1.7)\n",
            "Requirement already satisfied: torch==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.1.0)\n",
            "Requirement already satisfied: apex==0.9.10dev in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.9.10.dev0)\n",
            "Requirement already satisfied: GPUtil==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: scikit_learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (0.23.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.0.6)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (7.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (2.0.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.3.1->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (0.4.4)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (5.1.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1->-r requirements.txt (line 6)) (6.1.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.3->-r requirements.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.3->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (0.0.49)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (0.1.96)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 8)) (1.21.32)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->-r requirements.txt (line 9)) (0.17.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.1->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 12)) (5.2.1)\n",
            "Requirement already satisfied: py3nvml in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (0.2.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (0.18.2)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (11.0.3)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (1.3.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (3.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (2.3.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune_client==0.4.107->-r requirements.txt (line 13)) (3.1.27)\n",
            "Requirement already satisfied: matrix-client in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (13.11)\n",
            "Requirement already satisfied: keyring in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (23.5.0)\n",
            "Requirement already satisfied: twilio in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (7.8.0)\n",
            "Requirement already satisfied: yagmail>=0.11.214 in /usr/local/lib/python3.7/dist-packages (from knockknock==0.1.7->-r requirements.txt (line 14)) (0.15.277)\n",
            "Requirement already satisfied: wtforms in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (3.0.1)\n",
            "Requirement already satisfied: pyramid-mailer in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (0.15.1)\n",
            "Requirement already satisfied: pyramid>1.1.2 in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (2.0)\n",
            "Requirement already satisfied: wtforms-recaptcha in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (0.3.2)\n",
            "Requirement already satisfied: zope.sqlalchemy in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (1.6)\n",
            "Requirement already satisfied: velruse>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (1.1.1)\n",
            "Requirement already satisfied: cryptacular in /usr/local/lib/python3.7/dist-packages (from apex==0.9.10dev->-r requirements.txt (line 16)) (1.6.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.23.2->-r requirements.txt (line 18)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.23.2->-r requirements.txt (line 18)) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (4.11.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13)) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune_client==0.4.107->-r requirements.txt (line 13)) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: plaster-pastedeploy in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (0.7)\n",
            "Requirement already satisfied: webob>=1.8.3 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.8.7)\n",
            "Requirement already satisfied: venusian>=1.0 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (3.0.0)\n",
            "Requirement already satisfied: zope.interface>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (5.4.0)\n",
            "Requirement already satisfied: translationstring>=0.4 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.4)\n",
            "Requirement already satisfied: plaster in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.0)\n",
            "Requirement already satisfied: hupper>=1.5 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (1.10.3)\n",
            "Requirement already satisfied: zope.deprecation>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (1.25.11)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r requirements.txt (line 9)) (2.6.3)\n",
            "Requirement already satisfied: anykeystore in /usr/local/lib/python3.7/dist-packages (from velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16)) (0.2)\n",
            "Requirement already satisfied: python3-openid in /usr/local/lib/python3.7/dist-packages (from velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16)) (3.2.0)\n",
            "Requirement already satisfied: premailer in /usr/local/lib/python3.7/dist-packages (from yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 8)) (1.24.32)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 8)) (0.5.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.6)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (5.17.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.0.3)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (3.17.6)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (2.7.4)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.2)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (5.4.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.11.1)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.3.8)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.5.1)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (2.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.1.4)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.2.0)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (20.11.0)\n",
            "Requirement already satisfied: pbkdf2 in /usr/local/lib/python3.7/dist-packages (from cryptacular->apex==0.9.10dev->-r requirements.txt (line 16)) (1.3)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.5.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis==0.5.1->-r requirements.txt (line 6)) (0.2.5)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from isoduration->jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune_client==0.4.107->-r requirements.txt (line 13)) (1.2.2)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (0.7.1)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.7/dist-packages (from keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (3.3.1)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (36.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->knockknock==0.1.7->-r requirements.txt (line 14)) (2.21)\n",
            "Requirement already satisfied: PasteDeploy>=2.0 in /usr/local/lib/python3.7/dist-packages (from plaster-pastedeploy->pyramid>1.1.2->apex==0.9.10dev->-r requirements.txt (line 16)) (2.1.1)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (1.1.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (4.2.2)\n",
            "Requirement already satisfied: cssutils in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (2.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from premailer->yagmail>=0.11.214->knockknock==0.1.7->-r requirements.txt (line 14)) (4.2.6)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.7/dist-packages (from py3nvml->neptune_client==0.4.107->-r requirements.txt (line 13)) (0.12.0)\n",
            "Requirement already satisfied: transaction in /usr/local/lib/python3.7/dist-packages (from pyramid-mailer->apex==0.9.10dev->-r requirements.txt (line 16)) (3.0.1)\n",
            "Requirement already satisfied: repoze.sendmail>=4.1 in /usr/local/lib/python3.7/dist-packages (from pyramid-mailer->apex==0.9.10dev->-r requirements.txt (line 16)) (4.4.1)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14)) (3.6.3)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14)) (6.1)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot->knockknock==0.1.7->-r requirements.txt (line 14)) (1.5.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from python3-openid->velruse>=1.0.3->apex==0.9.10dev->-r requirements.txt (line 16)) (0.7.1)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.7/dist-packages (from wtforms->apex==0.9.10dev->-r requirements.txt (line 16)) (2.0.1)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9 in /usr/local/lib/python3.7/dist-packages (from zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16)) (1.4.32)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=0.9->zope.sqlalchemy->apex==0.9.10dev->-r requirements.txt (line 16)) (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlq-y7f7nxdP"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec('Data/glove.42B.300d.txt', 'Data/glove.42B.300d_w2v.txt')\n",
        "word2vecmodel1 = KeyedVectors.load_word2vec_format('Data/glove.42B.300d_w2v.txt', binary=False)\n",
        "word2vecmodel1.save(\"Data/word2vec.model\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrVyCVHdlWSc",
        "outputId": "df8633a0-1412-4850-df67-cc9d93ab2d15"
      },
      "source": [
        "import gc\n",
        "del word2vecmodel1\n",
        "gc.collect()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HS3NNHzmX64"
      },
      "source": [
        "!rm Data/glove.42B.300d.txt\n",
        "!rm Data/glove.42B.300d_w2v.txt"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smH302_aidUt",
        "outputId": "167f761f-a1aa-473f-808e-98bef28bd357"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.8\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.3 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53QcDc7AqkK5",
        "outputId": "c3f5ef91-a319-4880-c4eb-03c36d22e7c7"
      },
      "source": [
        "from manual_training_inference import *"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIKH2h5hzwcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562632d1-16b7-4d8a-ec8d-d2c3c0c20d68"
      },
      "source": [
        "path_file='best_model_json/bestModel_birnnscrat.json'\n",
        "with open(path_file,mode='r') as f:\n",
        "    params = json.load(f)\n",
        "for key in params:\n",
        "    if params[key] == 'True':\n",
        "          params[key]=True\n",
        "    elif params[key] == 'False':\n",
        "          params[key]=False\n",
        "    if( key in ['batch_size','num_classes','hidden_size','supervised_layer_pos','num_supervised_heads','random_seed','max_length']):\n",
        "        if(params[key]!='N/A'):\n",
        "            params[key]=int(params[key])\n",
        "        \n",
        "    if((key == 'weights') and (params['auto_weights']==False)):\n",
        "        params[key] = ast.literal_eval(params[key])\n",
        "\n",
        "##### change in logging to output the results to neptune\n",
        "params['logging']='local'\n",
        "params['device']='cpu'\n",
        "params['best_params']=False\n",
        "\n",
        "if torch.cuda.is_available() and params['device']=='cuda':    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('Since you dont want to use GPU, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "    \n",
        "#### Few handy keys that you can directly change.\n",
        "params['variance']=1\n",
        "params['epochs']=5\n",
        "params['to_save']=True\n",
        "params['num_classes']=2\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "        \n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Since you dont want to use GPU, using the CPU instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 39/20148 [00:00<00:52, 380.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_data 20148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20148/20148 [00:39<00:00, 508.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_error: 0\n",
            "no_majority: 919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 910/15383 [00:00<00:03, 4498.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:03<00:00, 4384.88it/s]\n",
            "  3%|▎         | 522/15383 [00:00<00:02, 5215.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22236, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:03<00:00, 4908.04it/s]\n",
            "100%|██████████| 1922/1922 [00:00<00:00, 4801.32it/s]\n",
            "100%|██████████| 1924/1924 [00:00<00:00, 4742.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total dataset size: 19229\n",
            "[1.2301791 0.8423818]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [03:36,  2.22it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.3132795623335\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:49,  9.73it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.64\n",
            " Fscore: 0.64\n",
            " Precision: 0.73\n",
            " Recall: 0.69\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:50\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06,  9.77it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.62\n",
            " Fscore: 0.61\n",
            " Precision: 0.72\n",
            " Recall: 0.67\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06,  9.52it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.62\n",
            " Fscore: 0.61\n",
            " Precision: 0.72\n",
            " Recall: 0.67\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "0.6090833014669614 0\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [03:50,  2.08it/s]\n",
            "2it [00:00, 10.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.1593047397796\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:49,  9.72it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.70\n",
            " Fscore: 0.70\n",
            " Precision: 0.76\n",
            " Recall: 0.74\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:50\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06,  9.97it/s]\n",
            "2it [00:00, 10.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.66\n",
            " Fscore: 0.65\n",
            " Precision: 0.72\n",
            " Recall: 0.70\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06, 10.00it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.67\n",
            " Fscore: 0.67\n",
            " Precision: 0.73\n",
            " Recall: 0.71\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "0.6535692320801031 0.6090833014669614\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [04:07,  1.95it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.0811206080066\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:48,  9.84it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.78\n",
            " Fscore: 0.78\n",
            " Precision: 0.80\n",
            " Recall: 0.80\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:49\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06, 10.08it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.69\n",
            " Fscore: 0.69\n",
            " Precision: 0.73\n",
            " Recall: 0.72\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06,  9.88it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.69\n",
            " Fscore: 0.69\n",
            " Precision: 0.73\n",
            " Recall: 0.72\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "0.6938777500284415 0.6535692320801031\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [04:20,  1.85it/s]\n",
            "2it [00:00, 10.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.0339653030998\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:48, 10.01it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.82\n",
            " Fscore: 0.82\n",
            " Precision: 0.83\n",
            " Recall: 0.84\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:48\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 10.27it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.70\n",
            " Fscore: 0.70\n",
            " Precision: 0.74\n",
            " Recall: 0.73\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06, 10.11it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.69\n",
            " Fscore: 0.69\n",
            " Precision: 0.73\n",
            " Recall: 0.72\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "0.7033616521678855 0.6938777500284415\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [04:23,  1.83it/s]\n",
            "2it [00:00, 10.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 294.9784104947984\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:48,  9.93it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.84\n",
            " Fscore: 0.84\n",
            " Precision: 0.85\n",
            " Recall: 0.86\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:49\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 10.60it/s]\n",
            "2it [00:00, 10.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.69\n",
            " Fscore: 0.69\n",
            " Precision: 0.73\n",
            " Recall: 0.72\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:06, 10.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.67\n",
            " Fscore: 0.67\n",
            " Precision: 0.71\n",
            " Recall: 0.70\n",
            " Roc Auc: 0.00\n",
            " Test took: 0:00:06\n",
            "best_val_fscore 0.7033616521678855\n",
            "best_test_fscore 0.691091613823973\n",
            "best_val_rocauc 0\n",
            "best_test_rocauc 0\n",
            "best_val_precision 0.7351268618784406\n",
            "best_test_precision 0.7257035939101157\n",
            "best_val_recall 0.7320397566660419\n",
            "best_test_recall 0.7209835125704893\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORBj47ArF8-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6f732e-e179-4f42-9607-5430ea47cfa5"
      },
      "source": [
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "if(params['num_classes']==2 and (params['auto_weights']==False)):\n",
        "      params['weights']=[1.0,1.0]\n",
        "        \n",
        "#for att_lambda in [0.001,0.01,0.1,1,10,100]\n",
        "train_model(params,device)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 50/20148 [00:00<00:40, 497.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_data 20148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20148/20148 [00:38<00:00, 519.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_error: 0\n",
            "no_majority: 919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 861/15383 [00:00<00:03, 4241.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:03<00:00, 4294.31it/s]\n",
            "  3%|▎         | 449/15383 [00:00<00:03, 4484.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22236, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15383/15383 [00:03<00:00, 5000.97it/s]\n",
            "100%|██████████| 1922/1922 [00:00<00:00, 4844.54it/s]\n",
            "100%|██████████| 1924/1924 [00:00<00:00, 4923.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total dataset size: 19229\n",
            "[1.0796857 0.8201194 1.1703163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [03:37,  2.21it/s]\n",
            "2it [00:00, 10.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.6874583159068\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:44, 10.83it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.62\n",
            " Fscore: 0.62\n",
            " Precision: 0.65\n",
            " Recall: 0.61\n",
            " Roc Auc: 0.80\n",
            " Test took: 0:00:45\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 10.92it/s]\n",
            "2it [00:00, 10.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.60\n",
            " Fscore: 0.59\n",
            " Precision: 0.64\n",
            " Recall: 0.58\n",
            " Roc Auc: 0.78\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 11.09it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.60\n",
            " Fscore: 0.59\n",
            " Precision: 0.63\n",
            " Recall: 0.58\n",
            " Roc Auc: 0.78\n",
            " Test took: 0:00:06\n",
            "0.5928381017318737 0\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [03:42,  2.16it/s]\n",
            "2it [00:00, 10.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.51231660267916\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:43, 11.07it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.69\n",
            " Fscore: 0.67\n",
            " Precision: 0.69\n",
            " Recall: 0.67\n",
            " Roc Auc: 0.84\n",
            " Test took: 0:00:44\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 11.19it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.64\n",
            " Fscore: 0.62\n",
            " Precision: 0.64\n",
            " Recall: 0.62\n",
            " Roc Auc: 0.80\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 11.10it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.65\n",
            " Fscore: 0.62\n",
            " Precision: 0.64\n",
            " Recall: 0.62\n",
            " Roc Auc: 0.80\n",
            " Test took: 0:00:06\n",
            "0.6166377960748289 0.5928381017318737\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [04:02,  1.98it/s]\n",
            "2it [00:00, 10.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.4182916906916\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:43, 11.17it/s]\n",
            "2it [00:00, 11.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.75\n",
            " Fscore: 0.74\n",
            " Precision: 0.74\n",
            " Recall: 0.74\n",
            " Roc Auc: 0.89\n",
            " Test took: 0:00:43\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 11.24it/s]\n",
            "2it [00:00, 11.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.64\n",
            " Fscore: 0.64\n",
            " Precision: 0.64\n",
            " Recall: 0.64\n",
            " Roc Auc: 0.81\n",
            " Test took: 0:00:05\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 11.23it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.65\n",
            " Fscore: 0.64\n",
            " Precision: 0.64\n",
            " Recall: 0.64\n",
            " Roc Auc: 0.82\n",
            " Test took: 0:00:05\n",
            "0.6365513941496782 0.6166377960748289\n",
            "Saving model\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [04:09,  1.93it/s]\n",
            "2it [00:00, 10.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.3653224381984\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:43, 11.10it/s]\n",
            "2it [00:00, 11.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.76\n",
            " Fscore: 0.76\n",
            " Precision: 0.78\n",
            " Recall: 0.76\n",
            " Roc Auc: 0.92\n",
            " Test took: 0:00:44\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 10.92it/s]\n",
            "2it [00:00, 10.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.63\n",
            " Fscore: 0.63\n",
            " Precision: 0.66\n",
            " Recall: 0.62\n",
            " Roc Auc: 0.81\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 10.90it/s]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.62\n",
            " Fscore: 0.62\n",
            " Precision: 0.65\n",
            " Recall: 0.61\n",
            " Roc Auc: 0.81\n",
            " Test took: 0:00:06\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [04:11,  1.91it/s]\n",
            "2it [00:00, 10.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_train_loss 295.29857389644377\n",
            "model previously passed\n",
            "Running eval on  train ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "481it [00:42, 11.20it/s]\n",
            "2it [00:00, 11.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.81\n",
            " Fscore: 0.81\n",
            " Precision: 0.81\n",
            " Recall: 0.81\n",
            " Roc Auc: 0.93\n",
            " Test took: 0:00:43\n",
            "model previously passed\n",
            "Running eval on  val ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 11.16it/s]\n",
            "2it [00:00, 10.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.64\n",
            " Fscore: 0.63\n",
            " Precision: 0.63\n",
            " Recall: 0.63\n",
            " Roc Auc: 0.79\n",
            " Test took: 0:00:06\n",
            "model previously passed\n",
            "Running eval on  test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "61it [00:05, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.63\n",
            " Fscore: 0.62\n",
            " Precision: 0.62\n",
            " Recall: 0.62\n",
            " Roc Auc: 0.79\n",
            " Test took: 0:00:06\n",
            "best_val_fscore 0.6365513941496782\n",
            "best_test_fscore 0.6394014618815519\n",
            "best_val_rocauc 0.8117476454123843\n",
            "best_test_rocauc 0.8203029594853541\n",
            "best_val_precision 0.6370146992257849\n",
            "best_test_precision 0.6387270124237755\n",
            "best_val_recall 0.6361208687390083\n",
            "best_test_recall 0.6401887284784715\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkvPokoMg3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dcba88-21a0-448a-90fe-ad119ce68863"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "735"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34jLhxdQ5stq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b894a82f-abd8-4371-c3bb-57c9cc263975"
      },
      "source": [
        "!python testing_with_rational.py birnn_scrat 100\n",
        "!python testing_for_bias.py birnn_scrat 100"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x55b411ce8000 @  0x7f0d207e01e7 0x55b3d0495338 0x55b3d045fad7 0x55b3d05419be 0x55b3d0578832 0x55b3d046407e 0x55b3d0463fa9 0x55b3d051cc3d 0x55b3d05aa7b9 0x55b3d051a88e 0x55b3d0405840 0x55b3d046311c 0x55b3d0462ef0 0x55b3d04d7123 0x55b3d04647aa 0x55b3d04d28f6 0x55b3d04d1cdd 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d04d1723 0x55b3d059b812 0x55b3d059bb8d 0x55b3d059ba36 0x55b3d0573183 0x55b3d0572e2c 0x7f0d1f5cac87 0x55b3d0572d0a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x55b49af4e000 @  0x7f0d207e01e7 0x55b3d0495338 0x55b3d045fad7 0x55b3d05aa762 0x55b3d051a88e 0x55b3d0405840 0x55b3d046311c 0x55b3d0462ef0 0x55b3d04d7123 0x55b3d04647aa 0x55b3d04d28f6 0x55b3d04d1cdd 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d04d1723 0x55b3d059b812 0x55b3d059bb8d 0x55b3d059ba36 0x55b3d0573183 0x55b3d0572e2c 0x7f0d1f5cac87 0x55b3d0572d0a\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['hatespeech' 'normal' 'offensive'], y=['normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'offensive', 'hatespeech', 'offensive', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'normal', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'offensive', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'hatespeech', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'normal', 'hatespeech', 'normal', 'hatespeech', 'hatespeech', 'hatespeech', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal', 'offensive', 'hatespeech', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'offensive', 'offensive', 'normal', 'offensive', 'normal', 'hatespeech', 'normal', 'normal', 'offensive', 'hatespeech', 'normal', 'normal', 'hatespeech', 'offensive', 'normal', 'hatespeech', 'offensive', 'offensive', 'offensive', 'hatespeech', 'normal'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "total_data 1142\n",
            "100% 1142/1142 [00:02<00:00, 492.72it/s]\n",
            "100% 1142/1142 [00:00<00:00, 5045.04it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 36/36 [00:03<00:00, 10.90it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " Accuracy: 0.622\n",
            " Fscore: 0.457\n",
            " Precision: 0.520\n",
            " Recall: 0.411\n",
            " Test took: 0:00:03\n",
            "100% 1142/1142 [00:00<00:00, 2291.09it/s]\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x55b411ce8000 @  0x7f0d207e01e7 0x55b3d0495338 0x55b3d045fad7 0x55b3d05419be 0x55b3d0578832 0x55b3d046407e 0x55b3d0463fa9 0x55b3d051cc3d 0x55b3d05aa7b9 0x55b3d051a88e 0x55b3d0405840 0x55b3d046311c 0x55b3d0462ef0 0x55b3d04d7123 0x55b3d04647aa 0x55b3d04d28f6 0x55b3d04d1cdd 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d04d1723 0x55b3d059b812 0x55b3d059bb8d 0x55b3d059ba36 0x55b3d0573183 0x55b3d0572e2c 0x7f0d1f5cac87 0x55b3d0572d0a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x55b49af4e000 @  0x7f0d207e01e7 0x55b3d0495338 0x55b3d045fad7 0x55b3d05aa762 0x55b3d051a88e 0x55b3d0405840 0x55b3d046311c 0x55b3d0462ef0 0x55b3d04d7123 0x55b3d04647aa 0x55b3d04d28f6 0x55b3d04d1cdd 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d04d1723 0x55b3d059b812 0x55b3d059bb8d 0x55b3d059ba36 0x55b3d0573183 0x55b3d0572e2c 0x7f0d1f5cac87 0x55b3d0572d0a\n",
            "100% 1142/1142 [00:00<00:00, 4164.39it/s]\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 36/36 [00:03<00:00, 10.77it/s]\n",
            "100% 1142/1142 [00:01<00:00, 1073.36it/s]\n",
            "Since you dont want to use GPU, using the CPU instead.\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x55b411ce8000 @  0x7f0d207e01e7 0x55b3d0495338 0x55b3d045fad7 0x55b3d05419be 0x55b3d0578832 0x55b3d046407e 0x55b3d0463fa9 0x55b3d051cc3d 0x55b3d05aa7b9 0x55b3d051a88e 0x55b3d0405840 0x55b3d046311c 0x55b3d0462ef0 0x55b3d04d7123 0x55b3d04647aa 0x55b3d04d28f6 0x55b3d04d1cdd 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d04d1723 0x55b3d059b812 0x55b3d059bb8d 0x55b3d059ba36 0x55b3d0573183 0x55b3d0572e2c 0x7f0d1f5cac87 0x55b3d0572d0a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x55b49af4e000 @  0x7f0d207e01e7 0x55b3d0495338 0x55b3d045fad7 0x55b3d05aa762 0x55b3d051a88e 0x55b3d0405840 0x55b3d046311c 0x55b3d0462ef0 0x55b3d04d7123 0x55b3d04647aa 0x55b3d04d28f6 0x55b3d04d1cdd 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d046488a 0x55b3d04d3719 0x55b3d04d1a2e 0x55b3d04d1723 0x55b3d059b812 0x55b3d059bb8d 0x55b3d059ba36 0x55b3d0573183 0x55b3d0572e2c 0x7f0d1f5cac87 0x55b3d0572d0a\n",
            "100% 1142/1142 [00:00<00:00, 3790.32it/s]\n",
            "Saved/birnnscrat_lstm_64_3_100.pth\n",
            "Running eval on test data...\n",
            "100% 36/36 [00:03<00:00, 11.24it/s]\n",
            "\u001b[0m/usr/local/lib/python3.7/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "Reading english - 1grams ...\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x555859b62000 @  0x7f26de9991e7 0x555818d4d338 0x555818d17ad7 0x555818df99be 0x555818e30832 0x555818d1c07e 0x555818d1bfa9 0x555818dd4c3d 0x555818e627b9 0x555818dd288e 0x555818cbd840 0x555818d1b11c 0x555818d1aef0 0x555818d8f123 0x555818d1c7aa 0x555818d8a8f6 0x555818d89a2e 0x555818d1c88a 0x555818d8b719 0x555818d89a2e 0x555818d1c88a 0x555818d8b719 0x555818d89a2e 0x555818d89723 0x555818e53812 0x555818e53b8d 0x555818e53a36 0x555818e2b183 0x555818e2ae2c 0x7f26dd783c87 0x555818e2ad0a\n",
            "tcmalloc: large alloc 2300993536 bytes == 0x5558e2dc8000 @  0x7f26de9991e7 0x555818d4d338 0x555818d17ad7 0x555818e62762 0x555818dd288e 0x555818cbd840 0x555818d1b11c 0x555818d1aef0 0x555818d8f123 0x555818d1c7aa 0x555818d8a8f6 0x555818d89a2e 0x555818d1c88a 0x555818d8b719 0x555818d89a2e 0x555818d1c88a 0x555818d8b719 0x555818d89a2e 0x555818d89723 0x555818e53812 0x555818e53b8d 0x555818e53a36 0x555818e2b183 0x555818e2ae2c 0x7f26dd783c87 0x555818e2ad0a\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['non-toxic' 'toxic'], y=['non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'non-toxic', 'toxic', 'toxic', 'non-toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'non-toxic'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "total_data 1924\n",
            "100% 1924/1924 [00:03<00:00, 533.55it/s]\n",
            "100% 1924/1924 [00:00<00:00, 4934.15it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Saved/birnnscrat_lstm_64_2_100.pth\n",
            "Running eval on test data...\n",
            "100% 61/61 [00:05<00:00, 11.16it/s]\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SynOxIW5PY-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc7a9e0-147c-4043-fb44-835d75b8a04c"
      },
      "source": [
        "!ls explanations_dicts"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestModel_birnnscrat_100_explanation_top5.json\tbestModel_birnnscrat_bias.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DVEb9O3IlCd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Ka6ukjTC5M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1-wjKhvNrF5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Bias Calculation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAPagtSDQ9zo"
      },
      "source": [
        "from collections import Counter,defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFOmyTVIRJ7R"
      },
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess.dataCollect import get_annotated_data"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw2qaVXLROW4"
      },
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'Data/dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "params = {}\n",
        "\n",
        "# We need to load the dataset with the labels as 'toxic' and 'non-toxic'. \n",
        "# We consider hatespeech and offensive as toxic and normal as non-toxic.\n",
        "params['num_classes']=2  \n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDRrrN8CRRfg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "outputId": "3fff1576-6278-4e7a-a980-17c97b522ff6"
      },
      "source": [
        "data_all_labelled"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           post_id  \\\n",
              "0      1179055004553900032_twitter   \n",
              "1      1179063826874032128_twitter   \n",
              "2      1178793830532956161_twitter   \n",
              "3      1179088797964763136_twitter   \n",
              "4      1179085312976445440_twitter   \n",
              "...                            ...   \n",
              "20143                  9989999_gab   \n",
              "20144                  9990225_gab   \n",
              "20145                  9991681_gab   \n",
              "20146                  9992513_gab   \n",
              "20147                  9998729_gab   \n",
              "\n",
              "                                                    text  annotatorid1  \\\n",
              "0      [i, dont, think, im, getting, my, baby, them, ...             1   \n",
              "1      [we, cannot, continue, calling, ourselves, fem...             1   \n",
              "2                    [nawt, yall, niggers, ignoring, me]             4   \n",
              "3      [<user>, i, am, bit, confused, coz, chinese, p...             1   \n",
              "4      [this, bitch, in, whataburger, eating, a, burg...             4   \n",
              "...                                                  ...           ...   \n",
              "20143  [if, ur, still, on, twitter, tell, carlton, i,...           217   \n",
              "20144  [when, i, first, got, on, here, and, said, i, ...           220   \n",
              "20145  [was, macht, der, moslem, wenn, der, zion, geg...           206   \n",
              "20146  [it, is, awful, look, at, world, demographics,...           209   \n",
              "20147  [the, jewish, globalist, elite, have, only, im...           200   \n",
              "\n",
              "                   target1      label1  annotatorid2             target2  \\\n",
              "0                   [None]      normal             2              [None]   \n",
              "1                   [None]      normal             2              [None]   \n",
              "2                [African]      normal             2              [None]   \n",
              "3                  [Asian]  hatespeech             4             [Asian]   \n",
              "4       [Caucasian, Women]  hatespeech             2  [Women, Caucasian]   \n",
              "...                    ...         ...           ...                 ...   \n",
              "20143  [Men, Women, Other]   offensive           199              [None]   \n",
              "20144            [African]   offensive           223    [African, Other]   \n",
              "20145              [Islam]   offensive           203             [Other]   \n",
              "20146           [Hispanic]  hatespeech           253             [Asian]   \n",
              "20147     [African, Islam]  hatespeech           202     [Islam, Jewish]   \n",
              "\n",
              "           label2  annotatorid3                   target3      label3  \\\n",
              "0          normal             3                    [None]      normal   \n",
              "1          normal             3                    [None]      normal   \n",
              "2          normal             3                 [African]  hatespeech   \n",
              "3       offensive             3                   [Asian]  hatespeech   \n",
              "4      hatespeech             3        [Women, Caucasian]   offensive   \n",
              "...           ...           ...                       ...         ...   \n",
              "20143   offensive           215                    [None]      normal   \n",
              "20144   offensive           231                    [None]      normal   \n",
              "20145      normal           211                    [None]      normal   \n",
              "20146  hatespeech           222                   [Asian]   offensive   \n",
              "20147   offensive           207  [African, Islam, Jewish]   offensive   \n",
              "\n",
              "                                              rationales final_label  \n",
              "0                                                     []   non-toxic  \n",
              "1                                                     []   non-toxic  \n",
              "2                                                     []   non-toxic  \n",
              "3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
              "4      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic  \n",
              "...                                                  ...         ...  \n",
              "20143  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...       toxic  \n",
              "20144  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...       toxic  \n",
              "20145                                                 []   non-toxic  \n",
              "20146  [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...       toxic  \n",
              "20147  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...       toxic  \n",
              "\n",
              "[20148 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b048666-2def-4ebe-a039-d0fa4d457b4b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotatorid1</th>\n",
              "      <th>target1</th>\n",
              "      <th>label1</th>\n",
              "      <th>annotatorid2</th>\n",
              "      <th>target2</th>\n",
              "      <th>label2</th>\n",
              "      <th>annotatorid3</th>\n",
              "      <th>target3</th>\n",
              "      <th>label3</th>\n",
              "      <th>rationales</th>\n",
              "      <th>final_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1179055004553900032_twitter</td>\n",
              "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1179063826874032128_twitter</td>\n",
              "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1178793830532956161_twitter</td>\n",
              "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
              "      <td>4</td>\n",
              "      <td>[African]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[African]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1179088797964763136_twitter</td>\n",
              "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>4</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>3</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1179085312976445440_twitter</td>\n",
              "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
              "      <td>4</td>\n",
              "      <td>[Caucasian, Women]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>2</td>\n",
              "      <td>[Women, Caucasian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>3</td>\n",
              "      <td>[Women, Caucasian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20143</th>\n",
              "      <td>9989999_gab</td>\n",
              "      <td>[if, ur, still, on, twitter, tell, carlton, i,...</td>\n",
              "      <td>217</td>\n",
              "      <td>[Men, Women, Other]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>199</td>\n",
              "      <td>[None]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>215</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20144</th>\n",
              "      <td>9990225_gab</td>\n",
              "      <td>[when, i, first, got, on, here, and, said, i, ...</td>\n",
              "      <td>220</td>\n",
              "      <td>[African]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>223</td>\n",
              "      <td>[African, Other]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>231</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20145</th>\n",
              "      <td>9991681_gab</td>\n",
              "      <td>[was, macht, der, moslem, wenn, der, zion, geg...</td>\n",
              "      <td>206</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>203</td>\n",
              "      <td>[Other]</td>\n",
              "      <td>normal</td>\n",
              "      <td>211</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20146</th>\n",
              "      <td>9992513_gab</td>\n",
              "      <td>[it, is, awful, look, at, world, demographics,...</td>\n",
              "      <td>209</td>\n",
              "      <td>[Hispanic]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>253</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>222</td>\n",
              "      <td>[Asian]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20147</th>\n",
              "      <td>9998729_gab</td>\n",
              "      <td>[the, jewish, globalist, elite, have, only, im...</td>\n",
              "      <td>200</td>\n",
              "      <td>[African, Islam]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>202</td>\n",
              "      <td>[Islam, Jewish]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>207</td>\n",
              "      <td>[African, Islam, Jewish]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20148 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b048666-2def-4ebe-a039-d0fa4d457b4b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b048666-2def-4ebe-a039-d0fa4d457b4b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b048666-2def-4ebe-a039-d0fa4d457b4b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14K9-nzFRU8B"
      },
      "source": [
        "def generate_target_information(dataset):\n",
        "    final_target_output = defaultdict(list)\n",
        "    all_communities_selected = []\n",
        "    \n",
        "    for each in dataset.iterrows(): \n",
        "        # All the target communities tagged for this post\n",
        "        all_targets = each[1]['target1']+each[1]['target2']+each[1]['target3']  \n",
        "        community_dict = dict(Counter(all_targets))\n",
        "        \n",
        "        # Select only those communities which are present more than once.\n",
        "        for key in community_dict:\n",
        "            if community_dict[key]>1:  \n",
        "                final_target_output[each[1]['post_id']].append(key)\n",
        "                all_communities_selected.append(key)\n",
        "        \n",
        "        # If no community is selected based on majority voting then we don't select any community\n",
        "        if each[1]['post_id'] not in final_target_output:\n",
        "            final_target_output[each[1]['post_id']].append('None')\n",
        "            all_communities_selected.append(key)\n",
        "\n",
        "    return final_target_output, all_communities_selected"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEmm7AD9Ra13"
      },
      "source": [
        "target_information, all_communities_selected = generate_target_information(data_all_labelled)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPCh_pj3ReFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876c2aa2-d05d-4f19-863d-5603473b5524"
      },
      "source": [
        "community_count_dict = Counter(all_communities_selected)\n",
        "\n",
        "# We remove None and Other from dictionary\n",
        "community_count_dict.pop('None')\n",
        "community_count_dict.pop('Other')\n",
        "\n",
        "# For the bias calculation, we are considering the top 10 communites based on their count\n",
        "list_selected_community = [community for community, value in community_count_dict.most_common(10)]\n",
        "list_selected_community"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['African',\n",
              " 'Islam',\n",
              " 'Jewish',\n",
              " 'Homosexual',\n",
              " 'Women',\n",
              " 'Refugee',\n",
              " 'Arab',\n",
              " 'Caucasian',\n",
              " 'Asian',\n",
              " 'Hispanic']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRU3peSrRhVo"
      },
      "source": [
        "# Based on the top 10 communities, we filter the target_information\n",
        "# This will remove the other communities from the calculation\n",
        "\n",
        "final_target_information ={}\n",
        "for each in target_information:\n",
        "    temp = list(set(target_information[each])&set(list_selected_community))\n",
        "    if len(temp) == 0:\n",
        "        final_target_information[each] = None\n",
        "    else:\n",
        "        final_target_information[each] = temp"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEsvl0xaRkpw"
      },
      "source": [
        "# Add a new column 'final_target_category' which will contain the selected target community names\n",
        "data_all_labelled['final_target_category'] = data_all_labelled['post_id'].map(final_target_information)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0-Y6iMiRnxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b018e41-294c-4dcd-b166-d82dccf87ff0"
      },
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "postpost_id_divisions_path = './Data/post_id_divisions.json'\n",
        "\n",
        "with open(postpost_id_divisions_path, 'r') as fp:\n",
        "    post_id_dict=json.load(fp)\n",
        "\n",
        "data_all_labelled_bias = data_all_labelled[data_all_labelled['post_id'].isin(post_id_dict['test'])]\n",
        "data_all_labelled_bias"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           post_id  \\\n",
              "0      1179055004553900032_twitter   \n",
              "5                     13851720_gab   \n",
              "17     1178818409812746240_twitter   \n",
              "25                    19346774_gab   \n",
              "36     1165819983701643266_twitter   \n",
              "...                            ...   \n",
              "20105                  9773208_gab   \n",
              "20109                  9802943_gab   \n",
              "20117                  9826963_gab   \n",
              "20128                  9867117_gab   \n",
              "20130                  9869181_gab   \n",
              "\n",
              "                                                    text  annotatorid1  \\\n",
              "0      [i, dont, think, im, getting, my, baby, them, ...             1   \n",
              "5      [laura, loomer, raped, me, while, screaming, a...             1   \n",
              "17     [<user>, what, did, the, old, lady, do, was, s...             9   \n",
              "25     [as, much, as, i, appreciate, a, format, of, o...             9   \n",
              "36     [sex, be, so, good, a, bitch, be, slow, stroki...             4   \n",
              "...                                                  ...           ...   \n",
              "20105  [it, always, women, trying, this, shit, like, ...           200   \n",
              "20109  [because, women, would, never, lie, about, bei...           228   \n",
              "20117  [how, is, all, that, awesome, muslim, diversit...           228   \n",
              "20128  [well, my, dear, lgbtq, brothers, and, sisters...           207   \n",
              "20130  [but, it, on, hbo, more, violence, and, sex, c...           223   \n",
              "\n",
              "              target1      label1  annotatorid2   target2      label2  \\\n",
              "0              [None]      normal             2    [None]      normal   \n",
              "5            [Jewish]  hatespeech             2  [Jewish]  hatespeech   \n",
              "17             [None]      normal            10    [None]      normal   \n",
              "25             [None]      normal            13    [None]      normal   \n",
              "36            [Women]   offensive             7   [Women]   offensive   \n",
              "...               ...         ...           ...       ...         ...   \n",
              "20105         [Women]  hatespeech           202   [Women]   offensive   \n",
              "20109         [Women]   offensive           222   [Women]   offensive   \n",
              "20117         [Islam]   offensive           222   [Islam]   offensive   \n",
              "20128  [Islam, Other]   offensive           223   [Islam]  hatespeech   \n",
              "20130          [None]      normal           200    [None]      normal   \n",
              "\n",
              "       annotatorid3     target3      label3  \\\n",
              "0                 3      [None]      normal   \n",
              "5                 3    [Jewish]  hatespeech   \n",
              "17                4      [None]      normal   \n",
              "25                4  [Hispanic]   offensive   \n",
              "36               16      [None]      normal   \n",
              "...             ...         ...         ...   \n",
              "20105           203     [Women]   offensive   \n",
              "20109           209     [Women]      normal   \n",
              "20117           209     [Islam]   offensive   \n",
              "20128           231     [Islam]  hatespeech   \n",
              "20130           233      [None]      normal   \n",
              "\n",
              "                                              rationales final_label  \\\n",
              "0                                                     []   non-toxic   \n",
              "5      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,...       toxic   \n",
              "17                                                    []   non-toxic   \n",
              "25                                                    []   non-toxic   \n",
              "36     [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, ...       toxic   \n",
              "...                                                  ...         ...   \n",
              "20105  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...       toxic   \n",
              "20109  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...       toxic   \n",
              "20117  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...       toxic   \n",
              "20128  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       toxic   \n",
              "20130                                                 []   non-toxic   \n",
              "\n",
              "      final_target_category  \n",
              "0                      None  \n",
              "5                  [Jewish]  \n",
              "17                     None  \n",
              "25                     None  \n",
              "36                  [Women]  \n",
              "...                     ...  \n",
              "20105               [Women]  \n",
              "20109               [Women]  \n",
              "20117               [Islam]  \n",
              "20128               [Islam]  \n",
              "20130                  None  \n",
              "\n",
              "[1924 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-021b1f99-e094-4b24-a9c4-d299fe081d95\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>text</th>\n",
              "      <th>annotatorid1</th>\n",
              "      <th>target1</th>\n",
              "      <th>label1</th>\n",
              "      <th>annotatorid2</th>\n",
              "      <th>target2</th>\n",
              "      <th>label2</th>\n",
              "      <th>annotatorid3</th>\n",
              "      <th>target3</th>\n",
              "      <th>label3</th>\n",
              "      <th>rationales</th>\n",
              "      <th>final_label</th>\n",
              "      <th>final_target_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1179055004553900032_twitter</td>\n",
              "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>2</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>3</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13851720_gab</td>\n",
              "      <td>[laura, loomer, raped, me, while, screaming, a...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>2</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>3</td>\n",
              "      <td>[Jewish]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Jewish]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1178818409812746240_twitter</td>\n",
              "      <td>[&lt;user&gt;, what, did, the, old, lady, do, was, s...</td>\n",
              "      <td>9</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>10</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>4</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>19346774_gab</td>\n",
              "      <td>[as, much, as, i, appreciate, a, format, of, o...</td>\n",
              "      <td>9</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>13</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>4</td>\n",
              "      <td>[Hispanic]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1165819983701643266_twitter</td>\n",
              "      <td>[sex, be, so, good, a, bitch, be, slow, stroki...</td>\n",
              "      <td>4</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>7</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>16</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, ...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Women]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20105</th>\n",
              "      <td>9773208_gab</td>\n",
              "      <td>[it, always, women, trying, this, shit, like, ...</td>\n",
              "      <td>200</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>202</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>203</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Women]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20109</th>\n",
              "      <td>9802943_gab</td>\n",
              "      <td>[because, women, would, never, lie, about, bei...</td>\n",
              "      <td>228</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>222</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>209</td>\n",
              "      <td>[Women]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Women]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20117</th>\n",
              "      <td>9826963_gab</td>\n",
              "      <td>[how, is, all, that, awesome, muslim, diversit...</td>\n",
              "      <td>228</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>222</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>209</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Islam]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20128</th>\n",
              "      <td>9867117_gab</td>\n",
              "      <td>[well, my, dear, lgbtq, brothers, and, sisters...</td>\n",
              "      <td>207</td>\n",
              "      <td>[Islam, Other]</td>\n",
              "      <td>offensive</td>\n",
              "      <td>223</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>231</td>\n",
              "      <td>[Islam]</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>toxic</td>\n",
              "      <td>[Islam]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20130</th>\n",
              "      <td>9869181_gab</td>\n",
              "      <td>[but, it, on, hbo, more, violence, and, sex, c...</td>\n",
              "      <td>223</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>200</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>233</td>\n",
              "      <td>[None]</td>\n",
              "      <td>normal</td>\n",
              "      <td>[]</td>\n",
              "      <td>non-toxic</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1924 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-021b1f99-e094-4b24-a9c4-d299fe081d95')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-021b1f99-e094-4b24-a9c4-d299fe081d95 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-021b1f99-e094-4b24-a9c4-d299fe081d95');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT3uzWgcRqaf"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# The names of the files which will contain the bias output for each model\n",
        "bias_score_file_mapping={\n",
        "    'BiRNN-Attn':'bestModel_birnnscrat_bias.json',\n",
        "}\n",
        "\n",
        "# The parent folder path of the bias output files\n",
        "parent_path = './explanations_dicts/'\n",
        "\n",
        "# The bias methods that will be considered\n",
        "method_list = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "community_list = list(list_selected_community)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0H9nMZNR1v3"
      },
      "source": [
        "# This function is used to convert the classification into a [0-1] score \n",
        "# with a value of 0 meaning non-toxic and 1 meaning toxic\n",
        "def convert_to_score(label_name, label_dict):\n",
        "    if label_name=='non-toxic':\n",
        "        return 1-label_dict[label_name]\n",
        "    else:\n",
        "        return label_dict[label_name]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFyY8iVR4kn"
      },
      "source": [
        "# Function to divide the ids into postive or class class based on the method.\n",
        "def bias_evaluation_metric(dataset, method, community):\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "    if method=='subgroup':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "            else:\n",
        "                pass\n",
        "    elif method=='bpsn':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "#                 print(eachrow[1]['final_label'])\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    elif method=='bnsp':\n",
        "        for eachrow in dataset.iterrows():\n",
        "            if eachrow[1]['final_target_category'] == None:\n",
        "                continue\n",
        "            if community in eachrow[1]['final_target_category']:\n",
        "                if eachrow[1]['final_label'] !='non-toxic':\n",
        "                    positive_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                if eachrow[1]['final_label'] =='non-toxic':\n",
        "                    negative_ids.append(eachrow[1]['post_id'])\n",
        "                else:\n",
        "                    pass\n",
        "    else:\n",
        "        print('Incorrect option selected!!!')\n",
        "                \n",
        "    return {'positiveID':positive_ids, 'negativeID':negative_ids}"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-CBxRFR7YQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5f85d0edf3014cf2856a0da44ddd3d84",
            "c1711bdf4c934334ba1cd6a818f1c933",
            "7930f6cfe719448f8119d5d08b55cad2",
            "62110a53e0ff47589b76ae0419ab2e09",
            "f56bf6f9275842878b00bf09f1b25016",
            "bff3579b80ef465fadcd770541fec81a",
            "2f4288a2816245c88cde3ef65ea4f9fe",
            "6e97c34947cd48599fd7a6af06561261"
          ]
        },
        "outputId": "bc272410-da3f-43f8-c160-16b7b9c4a32f"
      },
      "source": [
        "final_bias_dictionary = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "# We load each of the model bias output file and compute the bias score using each method for all the community\n",
        "for each_model in tqdm(bias_score_file_mapping):\n",
        "    total_data ={}\n",
        "    with open(parent_path+bias_score_file_mapping[each_model]) as fp:\n",
        "        for line in fp:\n",
        "            data = json.loads(line)\n",
        "            total_data[data['annotation_id']] = data\n",
        "    for each_method in method_list:\n",
        "        for each_community in community_list:\n",
        "            community_data = bias_evaluation_metric(data_all_labelled_bias, each_method, each_community)\n",
        "            truth_values = []\n",
        "            prediction_values = []\n",
        "\n",
        "\n",
        "            label_to_value = {'toxic':1.0, 'non-toxic':0.0}\n",
        "            for each in community_data['positiveID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            for each in community_data['negativeID']:\n",
        "                truth_values.append(label_to_value[total_data[each]['ground_truth']])\n",
        "                prediction_values.append(convert_to_score(total_data[each]['classification'], total_data[each]['classification_scores']))\n",
        "\n",
        "            roc_output_value = roc_auc_score(truth_values, prediction_values)\n",
        "            final_bias_dictionary[each_model][each_method][each_community] = roc_output_value"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f85d0edf3014cf2856a0da44ddd3d84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uoz4r8JR-23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e55ce569-5f51-4f91-b834-9dfc90fc65bf"
      },
      "source": [
        "%precision 4"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'%.4f'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIQi-Am9SCIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8466186c-2da2-4d7c-cfdd-183d9a8284fb"
      },
      "source": [
        "# To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
        "power_value = -5\n",
        "num_communities = len(community_list)\n",
        "\n",
        "for each_model in final_bias_dictionary:\n",
        "    for each_method in final_bias_dictionary[each_model]:\n",
        "        temp_value =[]\n",
        "        for each_community in final_bias_dictionary[each_model][each_method]:\n",
        "            temp_value.append(pow(final_bias_dictionary[each_model][each_method][each_community], power_value))\n",
        "        print(each_model, each_method, pow(np.sum(temp_value)/num_communities, 1/power_value))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiRNN-Attn subgroup 0.6956067830365597\n",
            "BiRNN-Attn bpsn 0.6909845152432692\n",
            "BiRNN-Attn bnsp 0.6884024716725496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1-xKrKSFza"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cos2FyRyScI6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Calculate Explainability**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0n04ccES0G3"
      },
      "source": [
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import more_itertools as mit\n",
        "import os"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ37OLVZS8gB"
      },
      "source": [
        "# get_annotated_data method is used to load the dataset\n",
        "from Preprocess import *\n",
        "from Preprocess.dataCollect import *"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AZCJG3wS-ko"
      },
      "source": [
        "dict_data_folder={\n",
        "      '2':{'data_file':'Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
        "      '3':{'data_file':'Data/dataset.json','class_label':'Data/classes.npy'}\n",
        "}\n",
        "\n",
        "# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class). \n",
        "\n",
        "params = {}\n",
        "params['num_classes']=3\n",
        "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
        "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
        "\n",
        "data_all_labelled=get_annotated_data(params)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5DLppxPTAjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0875a2ea-e7a4-4c04-fe68-b0e4c0038303"
      },
      "source": [
        "# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.\n",
        "\n",
        "params_data={\n",
        "    'include_special':False,  #True is want to include <url> in place of urls if False will be removed\n",
        "    'bert_tokens':False, #True /False\n",
        "    'type_attention':'softmax', #softmax\n",
        "    'set_decay':0.1,\n",
        "    'majority':2,\n",
        "    'max_length':128,\n",
        "    'variance':10,\n",
        "    'window':4,\n",
        "    'alpha':0.5,\n",
        "    'p_value':0.8,\n",
        "    'method':'additive',\n",
        "    'decay':False,\n",
        "    'normalized':False,\n",
        "    'not_recollect':True,\n",
        "}\n",
        "\n",
        "\n",
        "if(params_data['bert_tokens']):\n",
        "    print('Loading BERT tokenizer...')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "else:\n",
        "    print('Loading Normal tokenizer...')\n",
        "    tokenizer=None"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Normal tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqwkmy9ITEvH"
      },
      "source": [
        "# Load the whole dataset and get the tokenwise rationales\n",
        "def get_training_data(data):\n",
        "    post_ids_list=[]\n",
        "    text_list=[]\n",
        "    attention_list=[]\n",
        "    label_list=[]\n",
        "    \n",
        "    final_binny_output = []\n",
        "    print('total_data',len(data))\n",
        "    for index,row in tqdm(data.iterrows(),total=len(data)):\n",
        "        annotation=row['final_label']\n",
        "        \n",
        "        text=row['text']\n",
        "        post_id=row['post_id']\n",
        "        annotation_list=[row['label1'],row['label2'],row['label3']]\n",
        "        tokens_all = list(row['text'])\n",
        "#         attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]\n",
        "        \n",
        "        if(annotation!= 'undecided'):\n",
        "            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)\n",
        "            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
        "\n",
        "    return final_binny_output"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2lkIo1ATHjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ac645f-ac51-4a61-9834-fef9749e7658"
      },
      "source": [
        "training_data=get_training_data(data_all_labelled)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 51/20148 [00:00<00:39, 503.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_data 20148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20148/20148 [00:37<00:00, 539.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxfQUvdTJzn"
      },
      "source": [
        "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "            \n",
        "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
        "def get_evidence(post_id, anno_text, explanations):\n",
        "    output = []\n",
        "\n",
        "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
        "    span_list = list(find_ranges(indexes))\n",
        "\n",
        "    for each in span_list:\n",
        "        if type(each)== int:\n",
        "            start = each\n",
        "            end = each+1\n",
        "        elif len(each) == 2:\n",
        "            start = each[0]\n",
        "            end = each[1]+1\n",
        "        else:\n",
        "            print('error')\n",
        "\n",
        "        output.append({\"docid\":post_id, \n",
        "              \"end_sentence\": -1, \n",
        "              \"end_token\": end, \n",
        "              \"start_sentence\": -1, \n",
        "              \"start_token\": start, \n",
        "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
        "    return output\n",
        "\n",
        "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
        "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):  \n",
        "    final_output = []\n",
        "    \n",
        "    if save_split:\n",
        "        train_fp = open(save_path+'train.jsonl', 'w')\n",
        "        val_fp = open(save_path+'val.jsonl', 'w')\n",
        "        test_fp = open(save_path+'test.jsonl', 'w')\n",
        "            \n",
        "    for tcount, eachrow in enumerate(dataset):\n",
        "        \n",
        "        temp = {}\n",
        "        post_id = eachrow[0]\n",
        "        post_class = eachrow[1]\n",
        "        anno_text_list = eachrow[2]\n",
        "        majority_label = eachrow[1]\n",
        "        \n",
        "        if majority_label=='normal':\n",
        "            continue\n",
        "        \n",
        "        all_labels = eachrow[4]\n",
        "        explanations = []\n",
        "        for each_explain in eachrow[3]:\n",
        "            explanations.append(list(each_explain))\n",
        "        \n",
        "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
        "        if method == 'union':\n",
        "            final_explanation = [any(each) for each in zip(*explanations)]\n",
        "            final_explanation = [int(each) for each in final_explanation]\n",
        "        \n",
        "            \n",
        "        temp['annotation_id'] = post_id\n",
        "        temp['classification'] = post_class\n",
        "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
        "        temp['query'] = \"What is the class?\"\n",
        "        temp['query_type'] = None\n",
        "        final_output.append(temp)\n",
        "        \n",
        "        if save_split:\n",
        "            if not os.path.exists(save_path+'docs'):\n",
        "                os.makedirs(save_path+'docs')\n",
        "            \n",
        "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
        "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
        "            \n",
        "            if post_id in id_division['train']:\n",
        "                train_fp.write(json.dumps(temp)+'\\n')\n",
        "            \n",
        "            elif post_id in id_division['val']:\n",
        "                val_fp.write(json.dumps(temp)+'\\n')\n",
        "            \n",
        "            elif post_id in id_division['test']:\n",
        "                test_fp.write(json.dumps(temp)+'\\n')\n",
        "            else:\n",
        "                print(post_id)\n",
        "    \n",
        "    if save_split:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "        \n",
        "    return final_output"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deYKnU3wTRJn"
      },
      "source": [
        "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
        "with open('./Data/post_id_divisions.json') as fp:\n",
        "    id_division = json.load(fp)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlA0iMeETjUd"
      },
      "source": [
        "!mkdir ./Data/Evaluation\n",
        "!mkdir ./Data/Evaluation/Model_Eval"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2XwjEPOTUaY"
      },
      "source": [
        "method = 'union'\n",
        "save_split = True\n",
        "save_path = './Data/Evaluation/Model_Eval/'  #The dataset in Eraser Format will be stored here.\n",
        "output_eraser = convert_to_eraser_format(training_data, method, save_split, save_path, id_division)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e76FcTICTXrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5cbeac-b425-4512-9c8e-07b18311e925"
      },
      "source": [
        "!ls Data/Evaluation/Model_Eval/"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docs  test.jsonl  train.jsonl  val.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9lpwdAeTaf_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMWLQB2uT28g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4bd54b6-9730-40a6-b8c8-8b62935e4483"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model_json\t\t\t     Parameters_description.md\n",
            "best_runs.sh\t\t\t     parameters_selection.py\n",
            "Bias_Calculation_NB.ipynb\t     Preprocess\n",
            "convert_to_word2vec.py\t\t     __pycache__\n",
            "Data\t\t\t\t     README.md\n",
            "eraserbenchmark\t\t\t     requirements.txt\n",
            "Example_HateExplain.ipynb\t     Saved\n",
            "Explainability_Calculation_NB.ipynb  TensorDataset\n",
            "explanations_dicts\t\t     testing_for_bias.py\n",
            "Figures\t\t\t\t     testing_with_lime.py\n",
            "LICENSE\t\t\t\t     testing_with_rational.py\n",
            "manual_training_inference.py\t     test_parallel.sh\n",
            "Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylHvxsmoUv7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6ae746-4227-4bb4-e396-70369085a32e"
      },
      "source": [
        "cd eraserbenchmark/"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HateXplain/eraserbenchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SajAK6cMUyt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903ced51-b7e8-4417-ab44-ae28791923ae"
      },
      "source": [
        "!ls "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_exploration.ipynb\tparams\t\t     README.md\t       requirements.txt\n",
            "LICENSE\t\t\trationale_benchmark  REPRODUCTION.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iudyL6lcXib"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzZLhJf-U8Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b419420-2dc8-43e7-b73f-e1462038f476"
      },
      "source": [
        "!PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test  --data_dir ../Data/Evaluation/Model_Eval --results ../explanations_dicts/bestModel_birnnscrat_100_explanation_top5.json --score_file ../model_explain_output.json"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2507 MainThread Error in instances: 0 instances fail validation: set()\n",
            "  4721 MainThread No sentence level predictions detected, skipping sentence-level diagnostic\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "{'classification_scores': {'accuracy': 0.6217162872154116,\n",
            "                           'aopc_thresholds': None,\n",
            "                           'comprehensiveness': 0.2899375320616098,\n",
            "                           'comprehensiveness_aopc': None,\n",
            "                           'comprehensiveness_aopc_points': None,\n",
            "                           'comprehensiveness_entropy': 0.06618782785757517,\n",
            "                           'comprehensiveness_kl': 0.6169633591014108,\n",
            "                           'prf': {'accuracy': 0.6217162872154116,\n",
            "                                   'hatespeech': {'f1-score': 0.776888888888889,\n",
            "                                                  'precision': 0.8229755178907722,\n",
            "                                                  'recall': 0.7356902356902357,\n",
            "                                                  'support': 594},\n",
            "                                   'macro avg': {'f1-score': 0.45722004357298474,\n",
            "                                                 'precision': 0.5202711185762033,\n",
            "                                                 'recall': 0.4112884727239958,\n",
            "                                                 'support': 1142},\n",
            "                                   'normal': {'f1-score': 0.0,\n",
            "                                              'precision': 0.0,\n",
            "                                              'recall': 0.0,\n",
            "                                              'support': 0},\n",
            "                                   'offensive': {'f1-score': 0.5947712418300654,\n",
            "                                                 'precision': 0.7378378378378379,\n",
            "                                                 'recall': 0.4981751824817518,\n",
            "                                                 'support': 548},\n",
            "                                   'weighted avg': {'f1-score': 0.6894979339079474,\n",
            "                                                    'precision': 0.7821213596867371,\n",
            "                                                    'recall': 0.6217162872154116,\n",
            "                                                    'support': 1142}},\n",
            "                           'sufficiency': 0.0014583442993005395,\n",
            "                           'sufficiency_aopc': None,\n",
            "                           'sufficiency_aopc_points': None,\n",
            "                           'sufficiency_entropy': 0.036175898599931006,\n",
            "                           'sufficiency_kl': 0.03766218336473326},\n",
            " 'iou_scores': [{'macro': {'f1': 0.22809206487116668,\n",
            "                           'p': 0.14788382953882084,\n",
            "                           'r': 0.49842381786339757},\n",
            "                 'micro': {'f1': 0.22626646747249762,\n",
            "                           'p': 0.147069209039548,\n",
            "                           'r': 0.49028840494408477},\n",
            "                 'threshold': 0.5}],\n",
            " 'rationale_prf': {'instance_macro': {'f1': 0.11631116013777998,\n",
            "                                      'p': 0.0785026269702282,\n",
            "                                      'r': 0.2640980735551665},\n",
            "                   'instance_micro': {'f1': 0.12033138666304496,\n",
            "                                      'p': 0.0782132768361582,\n",
            "                                      'r': 0.2607416127133608}},\n",
            " 'token_prf': {'instance_macro': {'f1': 0.5074295479278658,\n",
            "                                  'p': 0.6181990659661425,\n",
            "                                  'r': 0.6430023805548017},\n",
            "               'instance_micro': {'f1': 0.44232469993682877,\n",
            "                                  'p': 0.618114406779661,\n",
            "                                  'r': 0.34438323824513084}},\n",
            " 'token_soft_metrics': {'auprc': 0.8384250389403275,\n",
            "                        'average_precision': 0.8342252704971385,\n",
            "                        'roc_auc_score': 0.8541055365711129}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1eQENR4VLp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd1b262-4772-4e3b-f464-320ad33ca53b"
      },
      "source": [
        "# print the required results\n",
        "with open('../model_explain_output.json') as fp:\n",
        "    output_data = json.load(fp)\n",
        "\n",
        "print('\\nPlausibility')\n",
        "print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])\n",
        "print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])\n",
        "print('AUPRC :', output_data['token_soft_metrics']['auprc'])\n",
        "\n",
        "print('\\nFaithfulness')\n",
        "print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])\n",
        "print('Sufficiency', output_data['classification_scores']['sufficiency'])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Plausibility\n",
            "IOU F1 : 0.22809206487116668\n",
            "Token F1 : 0.5074295479278658\n",
            "AUPRC : 0.8384250389403275\n",
            "\n",
            "Faithfulness\n",
            "Comprehensiveness : 0.2899375320616098\n",
            "Sufficiency 0.0014583442993005395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND6DYOMxTU8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}